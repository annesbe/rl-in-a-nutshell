{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Welcome to the FUN track of WiDS reinforement learning in a nutshell tutorial!\n",
    "\n",
    "In this track, you will play with several RL libraries that provide\n",
    "- Standard environments to train and compare different algorithms\n",
    "- Easy-to-use pre-implemented algorithms\n",
    "\n",
    "Now let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym in c:\\anaconda3\\envs\\ml\\lib\\site-packages (0.12.1)\n",
      "Requirement already satisfied: requests>=2.0 in c:\\anaconda3\\envs\\ml\\lib\\site-packages (from gym) (2.19.1)\n",
      "Requirement already satisfied: numpy>=1.10.4 in c:\\anaconda3\\envs\\ml\\lib\\site-packages (from gym) (1.15.3)\n",
      "Requirement already satisfied: scipy in c:\\anaconda3\\envs\\ml\\lib\\site-packages (from gym) (1.1.0)\n",
      "Requirement already satisfied: pyglet>=1.2.0 in c:\\anaconda3\\envs\\ml\\lib\\site-packages (from gym) (1.2.4)\n",
      "Requirement already satisfied: six in c:\\anaconda3\\envs\\ml\\lib\\site-packages (from gym) (1.11.0)\n",
      "Requirement already satisfied: idna<2.8,>=2.5 in c:\\anaconda3\\envs\\ml\\lib\\site-packages (from requests>=2.0->gym) (2.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\anaconda3\\envs\\ml\\lib\\site-packages (from requests>=2.0->gym) (2019.3.9)\n",
      "Requirement already satisfied: urllib3<1.24,>=1.21.1 in c:\\anaconda3\\envs\\ml\\lib\\site-packages (from requests>=2.0->gym) (1.23)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\anaconda3\\envs\\ml\\lib\\site-packages (from requests>=2.0->gym) (3.0.4)\n",
      "Requirement already satisfied: stable_baselines in c:\\anaconda3\\envs\\ml\\lib\\site-packages (2.4.1)\n",
      "Requirement already satisfied: tqdm in c:\\anaconda3\\envs\\ml\\lib\\site-packages (from stable_baselines) (4.28.1)\n",
      "Requirement already satisfied: progressbar2 in c:\\anaconda3\\envs\\ml\\lib\\site-packages (from stable_baselines) (3.39.3)\n",
      "Requirement already satisfied: click in c:\\anaconda3\\envs\\ml\\lib\\site-packages (from stable_baselines) (7.0)\n",
      "Requirement already satisfied: joblib in c:\\anaconda3\\envs\\ml\\lib\\site-packages (from stable_baselines) (0.13.2)\n",
      "Requirement already satisfied: glob2 in c:\\anaconda3\\envs\\ml\\lib\\site-packages (from stable_baselines) (0.6)\n",
      "Requirement already satisfied: gym[atari,classic_control]>=0.10.9 in c:\\anaconda3\\envs\\ml\\lib\\site-packages (from stable_baselines) (0.12.1)\n",
      "Requirement already satisfied: zmq in c:\\anaconda3\\envs\\ml\\lib\\site-packages (from stable_baselines) (0.0.0)\n",
      "Requirement already satisfied: dill in c:\\anaconda3\\envs\\ml\\lib\\site-packages (from stable_baselines) (0.2.9)\n",
      "Requirement already satisfied: mpi4py in c:\\anaconda3\\envs\\ml\\lib\\site-packages (from stable_baselines) (2.0.0)\n",
      "Requirement already satisfied: cloudpickle>=0.5.5 in c:\\anaconda3\\envs\\ml\\lib\\site-packages (from stable_baselines) (0.6.1)\n",
      "Requirement already satisfied: matplotlib in c:\\anaconda3\\envs\\ml\\lib\\site-packages (from stable_baselines) (3.0.1)\n",
      "Requirement already satisfied: pandas in c:\\anaconda3\\envs\\ml\\lib\\site-packages (from stable_baselines) (0.23.4)\n",
      "Requirement already satisfied: seaborn in c:\\anaconda3\\envs\\ml\\lib\\site-packages (from stable_baselines) (0.9.0)\n",
      "Requirement already satisfied: opencv-python in c:\\anaconda3\\envs\\ml\\lib\\site-packages (from stable_baselines) (4.0.0.21)\n",
      "Requirement already satisfied: numpy in c:\\anaconda3\\envs\\ml\\lib\\site-packages (from stable_baselines) (1.15.3)\n",
      "Requirement already satisfied: scipy in c:\\anaconda3\\envs\\ml\\lib\\site-packages (from stable_baselines) (1.1.0)\n",
      "Requirement already satisfied: python-utils>=2.3.0 in c:\\anaconda3\\envs\\ml\\lib\\site-packages (from progressbar2->stable_baselines) (2.3.0)\n",
      "Requirement already satisfied: six in c:\\anaconda3\\envs\\ml\\lib\\site-packages (from progressbar2->stable_baselines) (1.11.0)\n",
      "Requirement already satisfied: pyglet>=1.2.0 in c:\\anaconda3\\envs\\ml\\lib\\site-packages (from gym[atari,classic_control]>=0.10.9->stable_baselines) (1.2.4)\n",
      "Requirement already satisfied: requests>=2.0 in c:\\anaconda3\\envs\\ml\\lib\\site-packages (from gym[atari,classic_control]>=0.10.9->stable_baselines) (2.19.1)\n",
      "Requirement already satisfied: atari-py>=0.1.4; extra == \"atari\" in c:\\anaconda3\\envs\\ml\\lib\\site-packages (from gym[atari,classic_control]>=0.10.9->stable_baselines) (0.1.7)\n",
      "Requirement already satisfied: Pillow; extra == \"atari\" in c:\\anaconda3\\envs\\ml\\lib\\site-packages (from gym[atari,classic_control]>=0.10.9->stable_baselines) (5.3.0)\n",
      "Requirement already satisfied: PyOpenGL; extra == \"atari\" in c:\\anaconda3\\envs\\ml\\lib\\site-packages (from gym[atari,classic_control]>=0.10.9->stable_baselines) (3.1.0)\n",
      "Requirement already satisfied: pyzmq in c:\\anaconda3\\envs\\ml\\lib\\site-packages (from zmq->stable_baselines) (17.1.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\anaconda3\\envs\\ml\\lib\\site-packages (from matplotlib->stable_baselines) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\anaconda3\\envs\\ml\\lib\\site-packages (from matplotlib->stable_baselines) (1.0.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in c:\\anaconda3\\envs\\ml\\lib\\site-packages (from matplotlib->stable_baselines) (2.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\anaconda3\\envs\\ml\\lib\\site-packages (from matplotlib->stable_baselines) (2.7.5)\n",
      "Requirement already satisfied: pytz>=2011k in c:\\anaconda3\\envs\\ml\\lib\\site-packages (from pandas->stable_baselines) (2018.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\anaconda3\\envs\\ml\\lib\\site-packages (from requests>=2.0->gym[atari,classic_control]>=0.10.9->stable_baselines) (2019.3.9)\n",
      "Requirement already satisfied: idna<2.8,>=2.5 in c:\\anaconda3\\envs\\ml\\lib\\site-packages (from requests>=2.0->gym[atari,classic_control]>=0.10.9->stable_baselines) (2.7)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\anaconda3\\envs\\ml\\lib\\site-packages (from requests>=2.0->gym[atari,classic_control]>=0.10.9->stable_baselines) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.24,>=1.21.1 in c:\\anaconda3\\envs\\ml\\lib\\site-packages (from requests>=2.0->gym[atari,classic_control]>=0.10.9->stable_baselines) (1.23)\n",
      "Requirement already satisfied: setuptools in c:\\anaconda3\\envs\\ml\\lib\\site-packages (from kiwisolver>=1.0.1->matplotlib->stable_baselines) (39.1.0)\n"
     ]
    }
   ],
   "source": [
    "# For Windows, please check https://towardsdatascience.com/how-to-install-openai-gym-in-a-windows-environment-338969e24d30\n",
    "# for dependency installation\n",
    "\n",
    "!pip install gym\n",
    "!pip install stable_baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAIGym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "from stable_baselines.common.policies import MlpPolicy\n",
    "from stable_baselines.common.vec_env import DummyVecEnv\n",
    "from stable_baselines import PPO2\n",
    "from stable_baselines import A2C\n",
    "from stable_baselines import DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Render the simulation of the model in the environment\n",
    "# For vectorized models like PPO and A2C, set use_vec_env to True\n",
    "def render(env_id, use_vec_env=False, model=None, max_step=500):\n",
    "    env = gym.make(env_id)\n",
    "    if use_vec_env:\n",
    "        env = DummyVecEnv([lambda: env])\n",
    "    \n",
    "    observation = env.reset()\n",
    "    \n",
    "    for _ in range(max_step):\n",
    "        env.render()\n",
    "        if (model==None): # Sample a random action from the action space if no model is provided\n",
    "            if use_vec_env:\n",
    "                action = [env.action_space.sample()]\n",
    "            else:\n",
    "                action = env.action_space.sample()\n",
    "        else:\n",
    "            action, _states = model.predict(observation)\n",
    "\n",
    "        observation, reward, done, info = env.step(action)\n",
    "\n",
    "        if done:\n",
    "            observation = env.reset()\n",
    "\n",
    "    if use_vec_env:\n",
    "        env.envs[0].close()\n",
    "    else:\n",
    "        env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment\n",
    "env_id = \"CartPole-v1\"\n",
    "\n",
    "# Training parameters\n",
    "policy = \"MlpPolicy\"\n",
    "max_train_step = 10000\n",
    "\n",
    "model_path = \"./models/CartPole_PPO.model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random agent\n",
    "render(env_id, model=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make life easier, we use a variation of the original OpenAI baselines: [stable baselines](https://github.com/hill-a/stable-baselines)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO2(policy, env_id).learn(max_train_step)\n",
    "model.save(model_path)\n",
    "del model # We will reload the saved model for rendering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO2(policy, env_id).load(model_path)\n",
    "render(env_id, use_vec_env=True, model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try another environment with another model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment\n",
    "env_id = \"LunarLander-v2\"\n",
    "\n",
    "# Training parameters\n",
    "policy = \"MlpPolicy\"\n",
    "max_train_step = 10000\n",
    "\n",
    "model_path = \"./models/LunarLander_A2C.model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = A2C(policy, env_id, ent_coef=0.1).learn(total_timesteps=100000)\n",
    "# Save the agent\n",
    "model.save(model_path)\n",
    "del model  # delete trained model to demonstrate loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained agent\n",
    "model = A2C(policy, env_id, ent_coef=0.1).load(model_path)\n",
    "\n",
    "# Enjoy trained agent\n",
    "render(env_id, use_vec_env=True, model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorforce\n",
    "\n",
    "[Tensorforce](https://github.com/tensorforce/tensorforce) is an open-source library that provides modulized APIs for reinforcement learning. As the name suggest, it is built on top of TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorforce.agents import PPOAgent\n",
    "from tensorforce.execution import Runner\n",
    "from tensorforce.contrib.openai_gym import OpenAIGym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an OpenAIgym environment\n",
    "env = OpenAIGym('CartPole-v1', visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    }
   ],
   "source": [
    "# Network as list of layers\n",
    "network_spec = [\n",
    "    dict(type='dense', size=32, activation='tanh'),\n",
    "    dict(type='dense', size=32, activation='tanh')\n",
    "]\n",
    "\n",
    "agent = PPOAgent(\n",
    "    states=env.states,\n",
    "    actions=env.actions,\n",
    "    network=network_spec,\n",
    "    batching_capacity=4096,\n",
    "    step_optimizer=dict(\n",
    "        type='adam',\n",
    "        learning_rate=1e-3\n",
    "    ),\n",
    "    optimization_steps=10,\n",
    "    # Model\n",
    "    scope='ppo',\n",
    "    discount=0.99,\n",
    "    entropy_regularization=0.01,\n",
    "    likelihood_ratio_clipping=0.2,\n",
    "#    summarizer=dict(directory=\"./board/\",\n",
    "#                    steps=50,\n",
    "#                    labels=['graph',\n",
    "#                            'configuration',\n",
    "#                            'gradients_scalar',\n",
    "#                            'regularization',\n",
    "#                            'inputs',\n",
    "#                            'losses'\n",
    "#                            'variables'\n",
    "#                           ])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callback function printing episode statistics\n",
    "def episode_finished(r):\n",
    "    print(\"Finished episode {ep} after {ts} timesteps (reward: {reward})\".format(ep=r.episode, ts=r.episode_timestep,\n",
    "                                                                                 reward=r.episode_rewards[-1]))\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished episode 1 after 13 timesteps (reward: 13.0)\n",
      "Finished episode 2 after 15 timesteps (reward: 15.0)\n",
      "Finished episode 3 after 25 timesteps (reward: 25.0)\n",
      "Finished episode 4 after 28 timesteps (reward: 28.0)\n",
      "Finished episode 5 after 16 timesteps (reward: 16.0)\n",
      "Finished episode 6 after 30 timesteps (reward: 30.0)\n",
      "Finished episode 7 after 13 timesteps (reward: 13.0)\n",
      "Finished episode 8 after 18 timesteps (reward: 18.0)\n",
      "Finished episode 9 after 10 timesteps (reward: 10.0)\n",
      "Finished episode 10 after 26 timesteps (reward: 26.0)\n",
      "Finished episode 11 after 27 timesteps (reward: 27.0)\n",
      "Finished episode 12 after 22 timesteps (reward: 22.0)\n",
      "Finished episode 13 after 23 timesteps (reward: 23.0)\n",
      "Finished episode 14 after 11 timesteps (reward: 11.0)\n",
      "Finished episode 15 after 23 timesteps (reward: 23.0)\n",
      "Finished episode 16 after 22 timesteps (reward: 22.0)\n",
      "Finished episode 17 after 21 timesteps (reward: 21.0)\n",
      "Finished episode 18 after 18 timesteps (reward: 18.0)\n",
      "Finished episode 19 after 53 timesteps (reward: 53.0)\n",
      "Finished episode 20 after 11 timesteps (reward: 11.0)\n",
      "Finished episode 21 after 23 timesteps (reward: 23.0)\n",
      "Finished episode 22 after 33 timesteps (reward: 33.0)\n",
      "Finished episode 23 after 16 timesteps (reward: 16.0)\n",
      "Finished episode 24 after 74 timesteps (reward: 74.0)\n",
      "Finished episode 25 after 32 timesteps (reward: 32.0)\n",
      "Finished episode 26 after 17 timesteps (reward: 17.0)\n",
      "Finished episode 27 after 17 timesteps (reward: 17.0)\n",
      "Finished episode 28 after 37 timesteps (reward: 37.0)\n",
      "Finished episode 29 after 28 timesteps (reward: 28.0)\n",
      "Finished episode 30 after 12 timesteps (reward: 12.0)\n",
      "Finished episode 31 after 18 timesteps (reward: 18.0)\n",
      "Finished episode 32 after 24 timesteps (reward: 24.0)\n",
      "Finished episode 33 after 10 timesteps (reward: 10.0)\n",
      "Finished episode 34 after 30 timesteps (reward: 30.0)\n",
      "Finished episode 35 after 19 timesteps (reward: 19.0)\n",
      "Finished episode 36 after 11 timesteps (reward: 11.0)\n",
      "Finished episode 37 after 40 timesteps (reward: 40.0)\n",
      "Finished episode 38 after 11 timesteps (reward: 11.0)\n",
      "Finished episode 39 after 64 timesteps (reward: 64.0)\n",
      "Finished episode 40 after 39 timesteps (reward: 39.0)\n",
      "Finished episode 41 after 27 timesteps (reward: 27.0)\n",
      "Finished episode 42 after 14 timesteps (reward: 14.0)\n",
      "Finished episode 43 after 52 timesteps (reward: 52.0)\n",
      "Finished episode 44 after 12 timesteps (reward: 12.0)\n",
      "Finished episode 45 after 19 timesteps (reward: 19.0)\n",
      "Finished episode 46 after 16 timesteps (reward: 16.0)\n",
      "Finished episode 47 after 14 timesteps (reward: 14.0)\n",
      "Finished episode 48 after 34 timesteps (reward: 34.0)\n",
      "Finished episode 49 after 28 timesteps (reward: 28.0)\n",
      "Finished episode 50 after 17 timesteps (reward: 17.0)\n",
      "Finished episode 51 after 17 timesteps (reward: 17.0)\n",
      "Finished episode 52 after 14 timesteps (reward: 14.0)\n",
      "Finished episode 53 after 16 timesteps (reward: 16.0)\n",
      "Finished episode 54 after 19 timesteps (reward: 19.0)\n",
      "Finished episode 55 after 23 timesteps (reward: 23.0)\n",
      "Finished episode 56 after 84 timesteps (reward: 84.0)\n",
      "Finished episode 57 after 18 timesteps (reward: 18.0)\n",
      "Finished episode 58 after 40 timesteps (reward: 40.0)\n",
      "Finished episode 59 after 11 timesteps (reward: 11.0)\n",
      "Finished episode 60 after 19 timesteps (reward: 19.0)\n",
      "Finished episode 61 after 43 timesteps (reward: 43.0)\n",
      "Finished episode 62 after 29 timesteps (reward: 29.0)\n",
      "Finished episode 63 after 30 timesteps (reward: 30.0)\n",
      "Finished episode 64 after 16 timesteps (reward: 16.0)\n",
      "Finished episode 65 after 14 timesteps (reward: 14.0)\n",
      "Finished episode 66 after 58 timesteps (reward: 58.0)\n",
      "Finished episode 67 after 25 timesteps (reward: 25.0)\n",
      "Finished episode 68 after 20 timesteps (reward: 20.0)\n",
      "Finished episode 69 after 18 timesteps (reward: 18.0)\n",
      "Finished episode 70 after 31 timesteps (reward: 31.0)\n",
      "Finished episode 71 after 23 timesteps (reward: 23.0)\n",
      "Finished episode 72 after 36 timesteps (reward: 36.0)\n",
      "Finished episode 73 after 26 timesteps (reward: 26.0)\n",
      "Finished episode 74 after 40 timesteps (reward: 40.0)\n",
      "Finished episode 75 after 15 timesteps (reward: 15.0)\n",
      "Finished episode 76 after 74 timesteps (reward: 74.0)\n",
      "Finished episode 77 after 10 timesteps (reward: 10.0)\n",
      "Finished episode 78 after 41 timesteps (reward: 41.0)\n",
      "Finished episode 79 after 27 timesteps (reward: 27.0)\n",
      "Finished episode 80 after 18 timesteps (reward: 18.0)\n",
      "Finished episode 81 after 35 timesteps (reward: 35.0)\n",
      "Finished episode 82 after 14 timesteps (reward: 14.0)\n",
      "Finished episode 83 after 39 timesteps (reward: 39.0)\n",
      "Finished episode 84 after 19 timesteps (reward: 19.0)\n",
      "Finished episode 85 after 55 timesteps (reward: 55.0)\n",
      "Finished episode 86 after 27 timesteps (reward: 27.0)\n",
      "Finished episode 87 after 79 timesteps (reward: 79.0)\n",
      "Finished episode 88 after 12 timesteps (reward: 12.0)\n",
      "Finished episode 89 after 23 timesteps (reward: 23.0)\n",
      "Finished episode 90 after 22 timesteps (reward: 22.0)\n",
      "Finished episode 91 after 26 timesteps (reward: 26.0)\n",
      "Finished episode 92 after 29 timesteps (reward: 29.0)\n",
      "Finished episode 93 after 21 timesteps (reward: 21.0)\n",
      "Finished episode 94 after 26 timesteps (reward: 26.0)\n",
      "Finished episode 95 after 50 timesteps (reward: 50.0)\n",
      "Finished episode 96 after 17 timesteps (reward: 17.0)\n",
      "Finished episode 97 after 31 timesteps (reward: 31.0)\n",
      "Finished episode 98 after 22 timesteps (reward: 22.0)\n",
      "Finished episode 99 after 67 timesteps (reward: 67.0)\n",
      "Finished episode 100 after 64 timesteps (reward: 64.0)\n",
      "Learning finished. Total episodes: 100. Average reward of last 100 episodes: 27.46.\n"
     ]
    }
   ],
   "source": [
    "# Create the runner\n",
    "runner = Runner(agent=agent, environment=env)\n",
    "\n",
    "# Start learning\n",
    "runner.run(episodes=100, max_episode_timesteps=200, episode_finished=episode_finished)\n",
    "\n",
    "# Print statistics\n",
    "print(\"Learning finished. Total episodes: {ep}. Average reward of last 100 episodes: {ar}.\".format(\n",
    "    ep=runner.episode,\n",
    "    ar=np.mean(runner.episode_rewards[-100:]))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "runner.agent.save_model(directory=\"./agents/\")\n",
    "runner.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
